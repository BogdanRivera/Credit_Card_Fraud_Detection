# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FB-mkLvSmgkGVBde5Ol6FLxeSqCfQFE1

# **Credit Card Fraud Detection using Scikit-Learn and Snap ML**

## Problem
    This project presents a solution of a finantial institution that predicts if a credit card transaction is fraudulent or not. The classification is a possitive class (1) it is fraud, otherwise it belongs to the negative class(0).

    To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model.
"""

# install the opendatasets package
!pip install opendatasets

import opendatasets as od

# download the dataset (this is a Kaggle dataset)
# during download you will be required to input your Kaggle username and password
od.download("https://www.kaggle.com/mlg-ulb/creditcardfraud")

"""<div id="import_libraries">
    <h2>Import Libraries</h2>
</div>

"""

# Snap ML is available on PyPI. To install it simply run the pip command below.
!pip install snapml

# Commented out IPython magic to ensure Python compatibility.
# Import the libraries we need to use in this project
from __future__ import print_function
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import roc_auc_score
import time
import warnings
warnings.filterwarnings('ignore')

"""<div id="dataset_analysis">
    <h2>Dataset Analysis</h2>
</div>

First, the data is read to visualize some data statistics.
"""

# read the input data
raw_data = pd.read_csv('creditcardfraud/creditcard.csv')
print("There are " + str(len(raw_data)) + " observations in the credit card fraud dataset.")
print("There are " + str(len(raw_data.columns)) + " variables in the dataset.")

# display the first rows in the dataset
raw_data.head()

"""This data is small, we can update the dataset with 10 replicas."""

n_replicas = 10

# inflate the original dataset
big_raw_data = pd.DataFrame(np.repeat(raw_data.values, n_replicas, axis=0), columns=raw_data.columns)

print("There are " + str(len(big_raw_data)) + " observations in the inflated credit card fraud dataset.")
print("There are " + str(len(big_raw_data.columns)) + " variables in the dataset.")

# display first rows in the new dataset
big_raw_data.head()

"""The rows in dataset represents a credit card transaction. The tarjet variable in this dataset is the class wich is a categorical variable.

Note: For confidentiality reasons, the original names of most features are anonymized V1, V2 .. V28. The values of these features are the result of a PCA transformation and are numerical. The feature 'Class' is the target variable and it takes two values: 1 in case of fraud and 0 otherwise.
"""

# get the set of distinct classes
labels = big_raw_data.Class.unique() #Obtain unique values for the target variable
print("Labels: ",labels)
# get the count of each class
sizes = big_raw_data.Class.value_counts().values #Obtain the size of values
print("Value Counts: ",sizes)
# plot the class value counts
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, autopct='%1.3f%%')
ax.set_title('Target Variable Value Counts')
plt.show()

"""As presented above, the "Class" variable contains two values: 0, which represents legitimate credit card transactions, and 1, which indicates fraudulent transactions. Therefore, it is necessary to address a binary classification problem. Additionally, the data set suffers from imbalance, since the classes of the target variable are not equally represented. For this reason, it is crucial to pay special attention when training or evaluating a model to achieve higher quality results.

One strategy to deal with this situation during training is to bias the model so that it pays more attention to samples belonging to the minority class. The models under study will be configured to consider the weights of the samples of each class during the training and adjustment process.

To see data distribution for amoun, we can plot it with a histogram. In this case we can see the minimum value and maximum value. Addicionaly, we can 'predict' what is the value of the 90% of the transactions that have an amount or less than that value.
"""

plt.hist(big_raw_data.Amount.values, 6, histtype='bar', facecolor='g')
plt.show()

print("Minimum amount value is ", np.min(big_raw_data.Amount.values))
print("Maximum amount value is ", np.max(big_raw_data.Amount.values))
print("90% of the transactions have an amount less or equal than ", np.percentile(raw_data.Amount.values, 90))

"""<div id="dataset_preprocessing">
    <h2>Dataset Preprocessing</h2>
</div>

Preparing data training:
"""

# standardize features by removing the mean and scaling to unit variance
big_raw_data.iloc[:, 1:30] = StandardScaler().fit_transform(big_raw_data.iloc[:, 1:30])
data_matrix = big_raw_data.values
print("Data matrix shape: ",data_matrix.shape)

# X: feature matrix (for this analysis, we exclude the Time variable from the dataset)
X = data_matrix[:, 1:30]

# y: labels vector
y = data_matrix[:, 30]

# data normalization
X = normalize(X, norm="l1")

# print the shape of the features matrix and the labels vector
print('X.shape=', X.shape, 'y.shape=', y.shape)

"""<div id="dataset_split">
    <h2>Dataset Train/Test Split</h2>
</div>

We use the method train_test_split for divide the data in train and test
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape)
print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape)

"""<div id="dt_sklearn">
    <h2>Build a Decision Tree Classifier model with Scikit-Learn</h2>
</div>

"""

# compute the sample weights to be used as input to the train routine so that
# it takes into account the class imbalance present in this dataset

w_train = compute_sample_weight('balanced', y_train)

# import the Decision Tree Classifier Model from scikit-learn
from sklearn.tree import DecisionTreeClassifier

# for reproducible output across multiple function calls, set random_state to a given integer value
sklearn_dt = DecisionTreeClassifier(max_depth=4, random_state=35)

# train a Decision Tree Classifier using scikit-learn
t0 = time.time()
sklearn_dt.fit(X_train, y_train, sample_weight=w_train)
sklearn_time = time.time()-t0
print("[Scikit-Learn] Training time (s):  {0:.5f}".format(sklearn_time))

"""In this case, we can see the time to train the model is 36.45s

<div id="dt_snapml">
    <h2>Build a Decision Tree Classifier model with Snap ML</h2>
</div>
"""

# import the Decision Tree Classifier Model from Snap ML
from snapml import DecisionTreeClassifier
# Snap ML offers a relation between cpu/gpu, unlike sklearn

# for reproducible output across multiple function calls, set random_state to a given integer value
snapml_dt = DecisionTreeClassifier(max_depth=4, random_state=45, n_jobs=4)

# train a Decision Tree Classifier model using Snap ML
t0 = time.time()
snapml_dt.fit(X_train, y_train, sample_weight=w_train)
snapml_time = time.time()-t0
print("[Snap ML] Training time (s):  {0:.5f}".format(snapml_time))

"""We can see the Snap ML model is faster in comparison with Sklearn.

<div id="svm_sklearn">
    <h2>Build a Support Vector Machine model with Scikit-Learn</h2>
</div>
"""

# import the linear Support Vector Machine (SVM) model from Scikit-Learn
from sklearn.svm import LinearSVC


sklearn_svm = LinearSVC(class_weight='balanced', random_state=31, loss="hinge", fit_intercept=False)

# train a linear Support Vector Machine model using Scikit-Learn
t0 = time.time()
sklearn_svm.fit(X_train, y_train)
sklearn_time = time.time() - t0
print("[Scikit-Learn] Training time (s):  {0:.2f}".format(sklearn_time))

"""<div id="svm_snap">
    <h2>Build a Support Vector Machine model with Snap ML</h2>
</div>
"""

# import the Support Vector Machine model (SVM) from Snap ML
from snapml import SupportVectorMachine

# in contrast to scikit-learn's LinearSVC, Snap ML offers multi-threaded CPU/GPU training of SVMs
# to use the GPU, set the use_gpu parameter to True
# snapml_svm = SupportVectorMachine(class_weight='balanced', random_state=25, use_gpu=True, fit_intercept=False)

# to set the number of threads used at training time, one needs to set the n_jobs parameter
snapml_svm = SupportVectorMachine(class_weight='balanced', random_state=25, n_jobs=4, fit_intercept=False)
# print(snapml_svm.get_params())

# train an SVM model using Snap ML
t0 = time.time()
model = snapml_svm.fit(X_train, y_train)
snapml_time = time.time() - t0
print("[Snap ML] Training time (s):  {0:.2f}".format(snapml_time))

"""In this case you can see again Snap ML is faster. The evaluation of the quality of the SVM models trained above using the hinge loss metric to compare:"""

sklearn_pred = sklearn_svm.decision_function(X_test)
snapml_pred = snapml_svm.decision_function(X_test)
from sklearn.metrics import hinge_loss
# evaluate the hinge loss from the predictions
loss_snapml = hinge_loss(y_test, snapml_pred)
print("[Snap ML] Hinge loss:   {0:.3f}".format(loss_snapml))

# evaluate the hinge loss metric from the predictions
loss_sklearn = hinge_loss(y_test, sklearn_pred)
print("[Scikit-Learn] Hinge loss:   {0:.3f}".format(loss_snapml))

"""Finally, we can predict any values given. Using the data information:"""

client = np.random.randint(0,len(big_raw_data))
print("We evaluate the client number: ",client)
value = snapml_dt.predict(X[[client]])
print(f"The client {client} belongs to the classification ",value[0])
print(f"For the real data, the client {client} belongs to the classification ",big_raw_data.iloc[client].Class)

